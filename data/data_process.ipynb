{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#directory for MIMIC-III data\n",
    "base_dir = './'\n",
    "\n",
    "admission_file = base_dir+'ADMISSIONS.csv'\n",
    "procedure_file = base_dir+'PROCEDURES_ICD.csv'\n",
    "prescriptions_file = base_dir+'PRESCRIPTIONS.csv'\n",
    "diagnoses_file = base_dir+'DIAGNOSES_ICD.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "admission_df = pd.read_csv(admission_file)\n",
    "# Convert column names to uppercase\n",
    "admission_df.columns = admission_df.columns.str.upper()\n",
    "\n",
    "# Convert time columns to datetime format\n",
    "admission_df.ADMITTIME = pd.to_datetime(admission_df.ADMITTIME,format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "admission_df.DISCHTIME = pd.to_datetime(admission_df.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admission_df.DEATHTIME = pd.to_datetime(admission_df.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "# Sort the DataFrame by SUBJECT_ID and ADMITTIME\n",
    "admission_df = admission_df.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "admission_df = admission_df.reset_index(drop=True)\n",
    "\n",
    "# Add columns for the next admission time and type for each subject\n",
    "admission_df['NEXT_ADMITTIME'] = admission_df.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admission_df['NEXT_ADMISSION_TYPE'] = admission_df.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "\n",
    "# Handle cases where the next admission type is 'ELECTIVE'\n",
    "rows = admission_df.NEXT_ADMISSION_TYPE=='ELECTIVE'\n",
    "admission_df.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
    "admission_df.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN\n",
    "\n",
    "admission_df = admission_df.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "\n",
    "admission_df[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admission_df.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method='bfill')\n",
    "admission_df['DAYS_NEXT_ADMIT'] = (admission_df.NEXT_ADMITTIME-admission_df.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
    "admission_df['OUTPUT_LABEL'] = (admission_df.DAYS_NEXT_ADMIT<30).astype('int')\n",
    "\n",
    "# Remove rows with ADMISSION_TYPE 'NEWBORN' and those with non-null DEATHTIME\n",
    "admission_df = admission_df[admission_df['ADMISSION_TYPE']!='NEWBORN']\n",
    "admission_df = admission_df[admission_df.DEATHTIME.isnull()]\n",
    "admission_df['DURATION'] = (admission_df['DISCHTIME']-admission_df['ADMITTIME']).dt.total_seconds()/(24*60*60)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ICD-9\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def expand_level2():\n",
    "    level2 = ['001-009', '010-018', '020-027', '030-041', '042', '045-049', '050-059', '060-066', '070-079', '080-088',\n",
    "              '090-099', '100-104', '110-118', '120-129', '130-136', '137-139', '140-149', '150-159', '160-165',\n",
    "              '170-176',\n",
    "              '176', '179-189', '190-199', '200-208', '209', '210-229', '230-234', '235-238', '239', '240-246',\n",
    "              '249-259',\n",
    "              '260-269', '270-279', '280-289', '290-294', '295-299', '300-316', '317-319', '320-327', '330-337', '338',\n",
    "              '339', '340-349', '350-359', '360-379', '380-389', '390-392', '393-398', '401-405', '410-414', '415-417',\n",
    "              '420-429', '430-438', '440-449', '451-459', '460-466', '470-478', '480-488', '490-496', '500-508',\n",
    "              '510-519',\n",
    "              '520-529', '530-539', '540-543', '550-553', '555-558', '560-569', '570-579', '580-589', '590-599',\n",
    "              '600-608',\n",
    "              '610-611', '614-616', '617-629', '630-639', '640-649', '650-659', '660-669', '670-677', '678-679',\n",
    "              '680-686',\n",
    "              '690-698', '700-709', '710-719', '720-724', '725-729', '730-739', '740-759', '760-763', '764-779',\n",
    "              '780-789',\n",
    "              '790-796', '797-799', '800-804', '805-809', '810-819', '820-829', '830-839', '840-848', '850-854',\n",
    "              '860-869',\n",
    "              '870-879', '880-887', '890-897', '900-904', '905-909', '910-919', '920-924', '925-929', '930-939',\n",
    "              '940-949',\n",
    "              '950-957', '958-959', '960-979', '980-989', '990-995', '996-999', 'V01-V91', 'V01-V09', 'V10-V19',\n",
    "              'V20-V29',\n",
    "              'V30-V39', 'V40-V49', 'V50-V59', 'V60-V69', 'V70-V82', 'V83-V84', 'V85', 'V86', 'V87', 'V88', 'V89',\n",
    "              'V90',\n",
    "              'V91', 'E000-E899', 'E000', 'E001-E030', 'E800-E807', 'E810-E819', 'E820-E825', 'E826-E829', 'E830-E838',\n",
    "              'E840-E845', 'E846-E849', 'E850-E858', 'E860-E869', 'E870-E876', 'E878-E879', 'E880-E888', 'E890-E899',\n",
    "              'E900-E909', 'E910-E915', 'E916-E928', 'E929', 'E930-E949', 'E950-E959', 'E960-E969', 'E970-E978',\n",
    "              'E980-E989', 'E990-E999']\n",
    "\n",
    "    # Create a dictionary to map level 3 codes to their corresponding level 2 code\n",
    "    level2_expand = {}\n",
    "    for i in level2:\n",
    "        tokens = i.split('-')\n",
    "        if i[0] == 'V':\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_expand[\"V%02d\" % j] = i\n",
    "        elif i[0] == 'E':\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_expand[\"E%03d\" % j] = i\n",
    "        else:\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0]), int(tokens[1]) + 1):\n",
    "                    level2_expand[\"%03d\" % j] = i\n",
    "    return level2_expand\n",
    "\n",
    "\n",
    "level3_dict = expand_level2()\n",
    "\n",
    "#transform codes to level 2 and level 3\n",
    "def transform_code(unique_code):\n",
    "    level2 = unique_code[:4] if unique_code[0]=='E' else unique_code[:3]\n",
    "    level3 = level3_dict[level2]\n",
    "    \n",
    "    return [level2,level3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V30    6339\n",
      "414    3576\n",
      "038    3389\n",
      "410    3307\n",
      "424    1721\n",
      "       ... \n",
      "919       1\n",
      "219       1\n",
      "597       1\n",
      "671       1\n",
      "259       1\n",
      "Name: diagnose_level2, Length: 651, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/3236583043.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  main_diag_df['diagnose_level2'],main_diag_df['diagnose_level3'] = zip(*main_diag_df['ICD9_CODE'].apply(transform_code))\n",
      "/tmp/ipykernel_2359920/3236583043.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  main_diag_df['diagnose_level2'],main_diag_df['diagnose_level3'] = zip(*main_diag_df['ICD9_CODE'].apply(transform_code))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diag_file = base_dir+'DIAGNOSES_ICD.csv'\n",
    "# Filter the dataframe to include only main diagnoses (SEQ_NUM == 1)\n",
    "diagnose_df = pd.read_csv(diag_file)\n",
    "main_diag_df = diagnose_df[diagnose_df['SEQ_NUM']==1]\n",
    "main_diag_df['diagnose_level2'],main_diag_df['diagnose_level3'] = zip(*main_diag_df['ICD9_CODE'].apply(transform_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/1846911743.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pres_df = pd.read_csv(prescriptions_file,dtype={'NDC':'category'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adm_df = pd.read_csv(admission_file)\n",
    "proce_df = pd.read_csv(procedure_file)\n",
    "pres_df = pd.read_csv(prescriptions_file,dtype={'NDC':'category'})\n",
    "diag_df = pd.read_csv(diagnoses_file)\n",
    "\n",
    "MIN_CODE_THRESHOLD = 50\n",
    "MEDIUM_CODE_THRESHOLD = 100\n",
    "LARGE_CODE_THEESHOLD = 500\n",
    "\n",
    "\n",
    "def construct_valid_subset(raw_df,column='ICD9_CODE',threshold=MIN_CODE_THRESHOLD,desc='filter desc:'):\n",
    "    base_df = raw_df[column].value_counts()\n",
    "\n",
    "    valid_code = base_df[base_df>=threshold].index.values\n",
    "    filtered_df = raw_df[raw_df[column].isin(valid_code)]\n",
    "    filtered_admission_ids = set(filtered_df['HADM_ID'].tolist())\n",
    "\n",
    "    return valid_code,filtered_admission_ids,filtered_df\n",
    "\n",
    "diag_codes,diag_admission_ids,diag_df = construct_valid_subset(diag_df,desc='valid diagnoses code base/num: ')\n",
    "proce_codes,proce_admission_ids,proce_df = construct_valid_subset(proce_df,desc='valid procedure code base/num: ')\n",
    "pres_codes,pres_admission_ids,pres_df = construct_valid_subset(pres_df,column='NDC',desc='valid prescription code base/num: ')\n",
    "\n",
    "common_admission_ids = diag_admission_ids & proce_admission_ids & pres_admission_ids\n",
    "\n",
    "diag_df = diag_df.groupby(['SUBJECT_ID','HADM_ID']).agg({'ICD9_CODE':lambda x:','.join(x)}).reset_index().rename(columns={'ICD9_CODE':'ICD9_DIAG'})\n",
    "common_diag_df = diag_df[diag_df['HADM_ID'].isin(common_admission_ids)]\n",
    "\n",
    "proce_df.ICD9_CODE = proce_df.ICD9_CODE.astype(str)\n",
    "proce_df = proce_df.groupby(['SUBJECT_ID','HADM_ID']).agg({'ICD9_CODE':lambda x:','.join(x)}).reset_index().rename(columns={'ICD9_CODE':'ICD9_PROCE'})\n",
    "\n",
    "pres_df.drop(index=pres_df[pres_df['NDC'] == '0'].index, axis=0, inplace=True)\n",
    "pres_df = pres_df.groupby(['SUBJECT_ID','HADM_ID']).agg({'NDC':lambda x:','.join(x)}).reset_index()\n",
    "\n",
    "\n",
    "common_df = pd.merge(common_diag_df,proce_df,on=['SUBJECT_ID','HADM_ID'])\n",
    "common_df = pd.merge(common_df,pres_df,on=['SUBJECT_ID','HADM_ID'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient statics: \n",
      "35241\n",
      "5330  patient has multi visits\n",
      "29911  patient has single visits\n",
      "admission statics:\n",
      "43738\n",
      "13827  admission can be formulate as a visit sequence\n",
      "29911  admission are single visits\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    for each admission, category it to only single visit or the visit can be formulated in a visit sequence\n",
    "'''\n",
    "print('patient statics: ')\n",
    "print(len(common_df.SUBJECT_ID.unique()))\n",
    "\n",
    "visit_num_df = common_df[['SUBJECT_ID','HADM_ID']].groupby('SUBJECT_ID').HADM_ID.unique().reset_index()\n",
    "visit_num_df['HADM_ID_LEN'] = visit_num_df['HADM_ID'].apply(lambda x:len(x))\n",
    "multi_subjects = visit_num_df[visit_num_df['HADM_ID_LEN']>1].SUBJECT_ID.unique()\n",
    "print(len(multi_subjects),' patient has multi visits')\n",
    "single_subjects = visit_num_df[visit_num_df['HADM_ID_LEN']==1].SUBJECT_ID.unique()\n",
    "print(len(single_subjects),' patient has single visits')\n",
    "\n",
    "print('admission statics:')\n",
    "print(len(common_df.HADM_ID.unique()))\n",
    "common_multi_df = common_df[common_df['SUBJECT_ID'].isin(multi_subjects)]\n",
    "print(len(common_multi_df),' admission can be formulate as a visit sequence')\n",
    "common_single_df = common_df[common_df['SUBJECT_ID'].isin(single_subjects)]\n",
    "print(len(common_single_df),' admission are single visits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41401', 'V3001', 'V3000', '41071', '0389', '4241']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/937086015.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_single_df['disease'] = common_single_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n"
     ]
    }
   ],
   "source": [
    "common_single_df['disease'] = common_single_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n",
    "aa = list(common_single_df['disease'].value_counts())\n",
    "\n",
    "disease_cohorts = list(common_single_df['disease'].value_counts().index.values[0:6])\n",
    "common_single_df = common_single_df[common_single_df['disease'].isin(disease_cohorts)]\n",
    "\n",
    "print(disease_cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/3812877453.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_admission_df['disease'] = train_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n",
      "/tmp/ipykernel_2359920/3812877453.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_admission_df['disease'] = valid_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n",
      "/tmp/ipykernel_2359920/3812877453.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_admission_df['disease'] = test_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    split all the visits into the train, valid, test dataset respectively\n",
    "'''\n",
    "import random,math\n",
    "\n",
    "single_admission_nums = len(common_single_df)\n",
    "\n",
    "#split all admissions into train,valid and test with ratio 0.6,0.2,0.2\n",
    "train_bound,valid_bound = math.floor(0.6*single_admission_nums),math.floor(0.8*single_admission_nums)\n",
    "all_admission_ids = common_single_df.HADM_ID.unique()\n",
    "\n",
    "random.shuffle(all_admission_ids)\n",
    "train_admission_ids = all_admission_ids[:train_bound]\n",
    "valid_admission_ids = all_admission_ids[train_bound:valid_bound]\n",
    "test_admission_ids = all_admission_ids[valid_bound:]\n",
    "assert single_admission_nums==len(train_admission_ids)+len(valid_admission_ids)+len(test_admission_ids)\n",
    "\n",
    "train_admission_df = common_single_df[common_single_df['HADM_ID'].isin(train_admission_ids)]\n",
    "valid_admission_df = common_single_df[common_single_df['HADM_ID'].isin(valid_admission_ids)]\n",
    "test_admission_df = common_single_df[common_single_df['HADM_ID'].isin(test_admission_ids)]\n",
    "\n",
    "train_admission_df['disease'] = train_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n",
    "valid_admission_df['disease'] = valid_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])\n",
    "test_admission_df['disease'] = test_admission_df['ICD9_DIAG'].apply(lambda x:x.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4771\n",
      "4771\n",
      "1591\n",
      "1590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_admission_df))\n",
    "train_admission_df = train_admission_df[train_admission_df['disease'].isin(disease_cohorts)]\n",
    "print(len(train_admission_df))\n",
    "test_admission_df = test_admission_df[test_admission_df['disease'].isin(disease_cohorts)]\n",
    "print(len(test_admission_df))\n",
    "valid_admission_df = valid_admission_df[valid_admission_df['disease'].isin(disease_cohorts)]\n",
    "print(len(valid_admission_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ndc2rxnorm_file = '../ndc_atc/ndc2rxnorm_mapping.txt'\n",
    "\n",
    "def ndc2atc(pres_df):\n",
    "    with open(ndc2rxnorm_file,'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    pres_df['ATC'] = pres_df['NDC'].map(lambda x:','.join([ndc2rxnorm[ndc] for ndc in x.split(',')]))\n",
    "    return pres_df\n",
    "\n",
    "train_admission_df = ndc2atc(train_admission_df)\n",
    "valid_admission_df = ndc2atc(valid_admission_df)\n",
    "test_admission_df = ndc2atc(test_admission_df)\n",
    "\n",
    "train_admission_file = './train_admissions.csv'\n",
    "valid_admission_file = './valid_admissions.csv'\n",
    "test_admission_file = './test_admissions.csv'\n",
    "train_admission_df.to_csv(train_admission_file,index=False)\n",
    "valid_admission_df.to_csv(valid_admission_file,index=False)\n",
    "test_admission_df.to_csv(test_admission_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/2537930031.py:15: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_sample = random.sample(set(hadm_ids)-set([admission_id]), 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct labels with similar and dissimilar counts:  23855 23855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359920/2537930031.py:15: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_sample = random.sample(set(hadm_ids)-set([admission_id]), 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct labels with similar and dissimilar counts:  7950 7950\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def construct_labels(test_admission_df,label_file):\n",
    "    similar_pairs,dis_similar_pairs= [],[]\n",
    "    by_disease_df = test_admission_df.groupby('disease')['HADM_ID'].unique().reset_index()\n",
    "    all_hadm_ids = test_admission_df['HADM_ID'].tolist()\n",
    "\n",
    "\n",
    "    for disease,hadm_ids in  zip(by_disease_df['disease'],by_disease_df['HADM_ID']):\n",
    "        if len(hadm_ids)==1:continue\n",
    "        hadm_ids = hadm_ids.tolist()\n",
    "\n",
    "        for admission_id in hadm_ids:\n",
    "            random_sample = random.sample(set(hadm_ids)-set([admission_id]), 5)\n",
    "            similar_pairs.extend([(admission_id,sample) for sample in random_sample])\n",
    "            other_hadm_ids = random.sample(list(set(all_hadm_ids)-set(hadm_ids)),5)\n",
    "            for o_admission_id in other_hadm_ids:\n",
    "                dis_similar_pairs.append((admission_id,o_admission_id))\n",
    "\n",
    "    print('construct labels with similar and dissimilar counts: ',len(similar_pairs),len(dis_similar_pairs))\n",
    "    with open(label_file,'w',encoding='utf-8') as writer:\n",
    "        writer.write('hadm_id\\t'+'hadm_id\\t'+'label\\n')\n",
    "        for similar_pair,dis_similar_pair in zip(similar_pairs,dis_similar_pairs):\n",
    "            writer.write(str(similar_pair[0])+'\\t'+str(similar_pair[1])+'\\t'+'1\\n')\n",
    "            writer.write(str(dis_similar_pair[0])+'\\t'+str(dis_similar_pair[1])+'\\t'+'0\\n')\n",
    "    return similar_pairs,dis_similar_pairs\n",
    "\n",
    "train_label_file = './train_label.csv'\n",
    "valid_label_file = './valid_label.csv'\n",
    "construct_labels(train_admission_df,train_label_file)\n",
    "construct_labels(valid_admission_df,valid_label_file)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n",
      "336\n",
      "1479\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "all_admission_df = pd.concat([train_admission_df,valid_admission_df,test_admission_df])\n",
    "all_diag_codes = []\n",
    "all_admission_df['ICD9_DIAG'].apply(lambda x:all_diag_codes.extend(x.split(',')))\n",
    "all_diag_codes = list(set(all_diag_codes))\n",
    "print(len(all_diag_codes))\n",
    "all_proce_codes = []\n",
    "all_admission_df['ICD9_PROCE'].apply(lambda x:all_proce_codes.extend(x.split(',')))\n",
    "all_proce_codes = list(set(all_proce_codes))\n",
    "print(len(all_proce_codes))\n",
    "all_atc_codes = []\n",
    "all_admission_df['ATC'].apply(lambda x:all_atc_codes.extend(x.split(',')))\n",
    "all_atc_codes = list(set(all_atc_codes))\n",
    "print(len(all_atc_codes))\n",
    "\n",
    "pickle.dump({'diag_codes':all_diag_codes,'proce_codes':all_proce_codes,'atc_codes':all_atc_codes},open('./vocab.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnose and procedure relation num:  41536\n",
      "diagnose and prescription relation num:  294670\n",
      "procedure and presciption relation num:  88962\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    construct all the knowledge graph with PMI value\n",
    "    the entity type: diagnose, procedure, prescription\n",
    "    the relation type: diagnose-procedure, diagnose-prescription, procedure-presciption\n",
    "'''\n",
    "from math import log\n",
    "\n",
    "def construct_ent_pairs(x,head_col,tail_col,all_pairs):\n",
    "    for head_ent in x[head_col].split(','):\n",
    "        for tail_ent in x[tail_col].split(','):\n",
    "            all_pairs.append(head_ent+','+tail_ent)\n",
    "\n",
    "\n",
    "'''\n",
    "    based on the valid pmi value, construct the relation\n",
    "'''\n",
    "def construct_relation(common_df,head_col,tail_col):\n",
    "    all_pairs = []\n",
    "    common_df.apply(construct_ent_pairs,axis=1,args=(head_col,tail_col,all_pairs))\n",
    "#     print(len(all_pairs))\n",
    "    entity_freq = {}\n",
    "    rel_pair_count = {}\n",
    "    for rel_pair in all_pairs:\n",
    "        head_ent,tail_ent = rel_pair.split(',')\n",
    "        if rel_pair not in rel_pair_count:\n",
    "            rel_pair_count[rel_pair] = 1\n",
    "        else:\n",
    "            rel_pair_count[rel_pair]+=1\n",
    "        if head_ent not in entity_freq:\n",
    "            entity_freq[head_ent] = 1\n",
    "        else:\n",
    "            entity_freq[head_ent]+=1\n",
    "        if tail_ent not in entity_freq:\n",
    "            entity_freq[tail_ent] = 1\n",
    "        else:\n",
    "            entity_freq[tail_ent]+=1\n",
    "\n",
    "    num_windows = len(all_pairs)\n",
    "    pmi_result = []\n",
    "    for rel_pair in rel_pair_count:\n",
    "        entities = rel_pair.split(',')\n",
    "        pmi = log((1.0*rel_pair_count[rel_pair]/num_windows)/(1.0*entity_freq[entities[0]]*entity_freq[entities[1]]/(num_windows*num_windows)))\n",
    "        if pmi<0:continue\n",
    "        pmi_result.append([entities[0],entities[1],pmi])\n",
    "    return pmi_result\n",
    "\n",
    "def write_relation(pmi_result,output_file):\n",
    "    with open(output_file,'x',encoding='utf-8') as writer:\n",
    "        writer.write('head ent'+'\\t'+'tail ent'+'\\t'+'pmi\\n')\n",
    "        for key in pmi_result:\n",
    "            writer.write(key[0]+'\\t'+key[1]+'\\t'+str(key[2])+'\\n')\n",
    "    print('relation file writing done...')\n",
    "\n",
    "\n",
    "diag_proce_rel = construct_relation(common_single_df,'ICD9_DIAG','ICD9_PROCE')\n",
    "print('diagnose and procedure relation num: ',len(diag_proce_rel))\n",
    "\n",
    "diag_pres_rel = construct_relation(common_single_df,'ICD9_DIAG','NDC')\n",
    "print('diagnose and prescription relation num: ',len(diag_pres_rel))\n",
    "\n",
    "proce_pres_rel = construct_relation(common_single_df,'ICD9_PROCE','NDC')\n",
    "print('procedure and presciption relation num: ',len(proce_pres_rel))\n",
    "\n",
    "write_relation(diag_proce_rel,'./diag_proce_rel.csv')\n",
    "write_relation(diag_pres_rel,'./diag_pres_rel.csv')\n",
    "write_relation(proce_pres_rel,'./proce_pres_rel.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f3890caa754d3044cdb79da81d23de89517aba85aa39c6945f27b74c06ace6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
